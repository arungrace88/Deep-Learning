{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO9PSz/YohT90Hb3nEV7+lk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE61jXdOHwRX",
        "colab_type": "text"
      },
      "source": [
        "# **LSTM** (Long Short Term Memory Networks)\n",
        "\n",
        "\n",
        "###               “the clouds are in the ???” \n",
        "### “I grew up in France… I speak fluent ??????.”\n",
        "\n",
        "In theory, RNNs are absolutely capable of handling such “long-term dependencies.” Sadly, in practice, RNNs don’t seem to be able to learn them. **[Hochreiter (1991) [German] and Bengio, et al. (1994)]**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqdex5vPKAqq",
        "colab_type": "text"
      },
      "source": [
        "# **LSTM Network**\n",
        "\n",
        "\n",
        "*   LSTMs are explicitly designed to avoid the long-term dependency problem.\n",
        "\n",
        "*   LSTM ( Long Short Term Memory ) Networks are called fancy recurrent neural networks with some additional features.\n",
        "\n",
        "Diagram for typical LSTM at time step 't':\n",
        "![LSTM at timestep t](https://miro.medium.com/max/1400/1*GHAGjzbM7LZz7bATVRdx4w.png)\n",
        "\n",
        "The difference between standard RNN and a LSTM could be viewed below:\n",
        "\n",
        "\n",
        "![Simple RNN](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png) **Standard RNN (Single layer)**![LSTM](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png) **LSTM (Four layers)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJCBB3xFLf45",
        "colab_type": "text"
      },
      "source": [
        "**LSTM Notations explained below:**\n",
        "\n",
        "![LSTM Notations](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcxEa1tuOoFI",
        "colab_type": "text"
      },
      "source": [
        "**Understanding LSTM**:\n",
        "\n",
        "*Memory state “C”*:\n",
        "\n",
        "![Memory State](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png)\n",
        "\n",
        "\n",
        "\n",
        "*   It runs straight down the entire chain, with only some minor linear interactions.\n",
        "*   It’s very easy for information to just flow along it unchanged.\n",
        "\n",
        "*   Information can be added or removed to the memory state, carefully regulated by structures called gates. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. **So how many gates are there in an LSTM????**\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-gate.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iov7AeVxfRVE",
        "colab_type": "text"
      },
      "source": [
        "Forget Gate :\n",
        "\n",
        "\n",
        "*   The first step in our LSTM is to decide what information we’re going to throw away from the memory state. \n",
        "*   This decision is made by a sigmoid layer called the “forget gate layer.”\n",
        "\n",
        "*   A 1 represents “completely keep this” while a 0 represents “completely \n",
        "get rid of this.”\n",
        "\n",
        "*   For Eg: When we see a new subject, we want to forget the gender of the old subject.\n",
        "\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlpv0aqogeFI",
        "colab_type": "text"
      },
      "source": [
        "Input Gate:\n",
        "\n",
        "The next step is to decide what new information we’re going to store in the memory state. This has two parts:\n",
        "\n",
        "\n",
        "*   First, a sigmoid layer decides which values we’ll update (it).\n",
        "*   Second, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state.\n",
        "\n",
        "Then we find (**it∗C~t**). This is the new candidate values, scaled by how much we decided to update each state value.\n",
        "\n",
        "For Eg: We’d want to add the gender of the new subject to the memory state, to replace the old one we’re forgetting.\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bq7zzvwi628",
        "colab_type": "text"
      },
      "source": [
        "It’s now time to update the old memory state, Ct−1, into the new memory state Ct.\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png)\n",
        "\n",
        "For Eg: This is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOigFMwskM56",
        "colab_type": "text"
      },
      "source": [
        "Output Gate:\n",
        "Finally, we need to decide what we’re going to output. This output will be based on our updated memory state, but will be a filtered version. \n",
        "\n",
        "\n",
        "*   First, we run a sigmoid layer which decides what parts of the memory state we’re going to output.\n",
        "*   Then, we put the memory state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9rtUbC1mkDF",
        "colab_type": "text"
      },
      "source": [
        "# **Variations of LSTM**\n",
        "\n",
        "One popular LSTM variant, introduced by Gers & Schmidhuber (2000), is adding “peephole connections.”\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-peepholes.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQfEmUfOnLLg",
        "colab_type": "text"
      },
      "source": [
        "Another variation is to use coupled forget and input gates. We only forget when we’re going to input something in its place. We only input new values to the state when we forget something older.\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-tied.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crc8RwJNoOvM",
        "colab_type": "text"
      },
      "source": [
        "Another variation is GRU (Gated Recurrent Unit).It combines the forget and input gates into a single “update gate.” It also merges the memory state and hidden state, and makes some other changes. The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.\n",
        "\n",
        "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-var-GRU.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmN2CRRGonL5",
        "colab_type": "text"
      },
      "source": [
        "**Which of these variants is best? **\n",
        "\n",
        "Jozefowicz, et al. (2015) tested more than ten thousand RNN architectures, finding some that worked better than LSTMs on certain tasks.\n",
        "\n",
        "**What else in future???**\n",
        "\n",
        "\n",
        "\n",
        "*   Attention - The idea is to let every step of an RNN pick information to look at from some larger collection of information (Xu, et al. (2015)).\n",
        "*   Grid LSTMs by Kalchbrenner, et al. (2015).\n",
        "\n",
        "\n",
        "*   RNNs in generative models – such as Gregor, et al. (2015), Chung, et al. (2015), or Bayer & Osendorfer (2015).\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}